%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Engineering problems / LaTeX Template
%		Semester 6
%		Institut d'Optique Graduate School
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	6N-IntNum-BlocVI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Created by:
%	Julien VILLEMEJANE - 16/jul/2024
% Fichier.sty modifié pour changer la police de caractère
%	
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Professional Newsletter Template
% LaTeX Template
% Version 1.0 (09/03/14)
%
% Created by:
% Bob Kerstetter (https://www.tug.org/texshowcase/) and extensively modified by:
% Vel (vel@latextemplates.com)
% 
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt,titlepage]{article} % The default font size is 10pt; 11pt and 12pt are alternatives

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{opto_elec_villemejane}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}



% Page de garde
\begin{titlepage}

\begin{center}
	\begin{minipage}{2.5cm}
	\begin{center}
		\includegraphics[width=8cm]{images/Logo-LEnsE.png}
	\end{center}
\end{minipage}\hfill
\begin{minipage}{10cm}
	\begin{center}
	\textbf{Institut d'Optique Graduate School }\\[0.1cm]
    \textbf{Interfaçage Numérique}


	\end{center}
\end{minipage}\hfill


\vspace{3cm}


{\huge \bfseries \textsc{Interfaçage Numérique}} \\[0.5cm]
{\large \bfseries Travaux Pratiques} \\[0.2cm]
Semestre 6

\vspace{1.5cm}
% Title
\rule{\linewidth}{0.3mm} \\[0.4cm]
{ \huge \bfseries\color{violet_iogs} Vision Industrielle \\[0.4cm] }
\rule{\linewidth}{0.3mm} \\[0.2cm]
{ \large \bfseries\color{violet_iogs} De la physique de la chaîne d'acquisition \\ à l'exploitation logicielle}
\rule{\linewidth}{0.3mm} \\[1cm]

4 séances

\bigskip

\begin{center}
	\includegraphics[width=0.8\textwidth]{images/vi_intro.png}
\end{center}

\vfill

\textit{Ce sujet est disponible au format électronique sur le site du LEnsE - https://lense.institutoptique.fr/ dans la rubrique Année / Première Année / Interfaçage Numérique S6 / Bloc 2 Vision Industrielle.}


% Bottom of the page
%{\textbf{\large {Année universitaire} 2024-2025}}

\end{center}
\end{titlepage}

\newpage
\strut % empty page

\newpage
\pagestyle{empty}

\begin{minipage}[c]{.25\linewidth}
	\includegraphics[width=5cm]{images/Logo-LEnsE.png}
\end{minipage} \hfill
\begin{minipage}[c]{.4\linewidth}

\begin{center}
\vspace{0.3cm}
{\Large \textsc{Interfaçage Numérique}}

\medskip

6N-047-SCI \qquad \textbf{\large Bloc VI}

\end{center}
\end{minipage}\hfill

\vspace{0.5cm}

\noindent \rule{\linewidth}{1pt}

{\noindent\Large  \rule[-7pt]{0pt}{30pt} \textbf{Vision Industrielle}}

\noindent \rule{\linewidth}{1pt}

\bigskip 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    A A V

La \textbf{vision industrielle} est une technologie qui permet à des machines d'\textbf{analyser automatiquement des scènes} pour \textbf{contrôler}, \textbf{guider} ou \textbf{inspecter} des objets sur des processus de production. Elle repose sur l'utilisation de \textbf{caméras}, d'\textbf{optique}, d'\textbf{éclairages} spécifiques (ou contraints), de \textbf{capteurs} et d'algorithmes de \textbf{traitement d'image}. 

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/vi_intro_2.png}
\end{center}

Elle a pour but de \textbf{prendre des décisions automatiques} (ou aider l'être humain dans sa prise de décision) vis-à-vis d'un (ou plusieurs) objet(s) dans une scène spécifique : détecter des défauts ou des irrégularités, compter ou trier..., en rejetant ou validant automatiquement des produits, tout en assurant une constance de la qualité et de la répétabilité des opérations. 


\begin{center}
	\includegraphics[width=0.6\textwidth]{images/vi_chaine.png}
\end{center}


\newpage
\textit{Ce sujet a été co-conçu par une équipe d'étudiant$\cdot$es - Joséphine BECHU, Justine GABRIEL et Paul CHENEAU - lors d'un projet DEPhI de 2A - 2025-2026 - et l'équipe pédagogique d'Interfaçage Numérique.}

\medskip

\section{Acquis d'Apprentissages Visés (AAV)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    A A V

À la fin de cette série de 4 séances, les étudiant$\cdot$es seront capable de :

\begin{itemize}
	\item Décomposer et paramétrer une chaîne de vision industrielle complète (du capteur au traitement de l'image),
	\item Comprendre les compromis physiques et numériques de chaque maillon,
	\item Réaliser un prototype d'inspection ou de mesure simple (tri d'objets),
	\item Justifier leurs choix de configuration (résolution, focale, éclairage...)
\end{itemize}

\medskip

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    Ressources

\section{Ressources}

Un tutoriel sur les bases d'OpenCV est disponible à l’adresse suivante : 

\href{https://iogs-lense-training.github.io/image-processing/}{https://iogs-lense-training.github.io/image-processing/}

Un \textbf{kit d'images} est disponible sur le site du LEnsE dans la rubrique \textit{Année / Première Année / Interfaçage Numérique S6 / Bloc 2 Caméra, Images et Interfaces / Images et OpenCV / Kit d'images}. 

Des \textbf{fichiers de fonctions} sont disponibles sur le site du LEnsE dans la rubrique\textit{Année / Première Année / Interfaçage Numérique S6 / Bloc 2 Caméra, Images et Interfaces / Images et OpenCV / Répertoire vers codes à tester}. 


Quelques \textbf{exemples} et explications sur les différents pré-traitements d'images est disponible sur le site du LEnsE dans la rubrique \textit{Année / Première Année / Interfaçage Numérique S6 / Bloc 2 Caméra, Images et Interfaces / Images et OpenCV / Image Processing with OpenCV}. 


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    Déroulement

\section{Déroulement}

Les sujets \textbf{TP1} et \textbf{TP2} se font en binôme et sont interchangeables (4 binômes commenceront par la TP1 lors de la première séance et les 4 autres binômes commenceront par le TP2).

Les sujets \textbf{TP3} et \textbf{TP4} se font par groupe de 4 étudiant$\cdot$es.


\subsection{TP1 - Banc de vision industrielle}

Le \textbf{TP1} se fera sur un banc de vision industrielle simple incluant une caméra, un objectif (focale : xx mm), un éclairage Effilux Ring-RGB et un ordinateur.

\medskip

\begin{itemize}
	\item {[20']} Prendre en main l'interface - caméra (temps d'intégration, histogramme) + éclairage RGB
	\item {[20']} Tester les outils de base proposés dans l'interface (coupe, lissage, seuillage)
	\item {[20']} Vérifier l'uniformité de l'éclairage sur objet uniforme - coupe dans l'image
	\item {[30']} Valider la linéarité de la caméra sur objet uniforme - pour différents temps d'intégration (en blanc et R/G/B)
	\item {[60']} Contrôler le champ de vision du système et sa résolution spatiale 
	\begin{itemize}
		\item placer une règle pour mesurer le FOV (Field of View)
		\item placer une mire pour mesurer la plus petite taille résoluble
		\item Modifier la profondeur binaire (8, 10, 12 bits) et voir l'impact sur la résolution spatiale / sur la qualité de l'image
		\item Revenir aux caractéristiques de l'objectif optique
	\end{itemize}
	\item {[30']} Contrôler le contraste du système en plaçant des mires en fonction du temps d'intégration, de l'éclairage
	\item {[60']} Analyser l'impact des propriétés des objets et des sources sur la valeur mesurée par la caméra
	\begin{itemize}
		\item Étudier les réflectances des cubes et le spectre des sources (fournis)
		\item Comparer les niveaux de gris obtenus sous différents éclairages des différents objets mis à disposition (cubes, formes...)
	\end{itemize}
	
\end{itemize}

\subsection{TP2 - Manipulations de base sous OpenCV}

Codes fournis (à modifier ?)

\begin{itemize}
	\item {[40']} Ouvrir une image - différence Gray/RGB
	\item {[20']} Calculer l'histogramme d'une image
	\item {[20']} Améliorer numériquement une image / contraste-luminosité
	\item {[20']} Appliquer un seuillage sur une image
	\item {[30']} Appliquer une érosion et une dilatation sur une image
	\item {[20']} Appliquer une ouverture et une fermeture sur une image
	\item {[20']} Appliquer un gradient sur une image
	\item {[30']} Calculer la FFT d'une image
	\item {[20']} Appliquer un filtre moyenneur sur une image
	\item {[20']} Appliquer un filtre passe-haut - Sobel
	
\end{itemize}



\subsection{TP3 - Manipulations avancées sous OpenCV / Limites du banc de vision}

Sous OpenCV :

\begin{itemize}
	\item {[40']} Détecter des formes 
	\item {[40']} Détecter des couleurs 
	\item {[40']} Recolorisation (matrice de changement de base)
	
\end{itemize}

Segmentation de l'image ?

\medskip

Sur le banc de VI :

\begin{itemize}
	\item {[40']} Calibration de la chaîne sur les cubes de couleurs (couleur)
	\item {[40']} Calibration de la chaîne sur des formes colorées
	\item {[40']} Calibration de la chaîne pour des mesures de distance
\end{itemize}



\subsection{TP4 - Détection d'objets - Contrôle qualité}
	
Le but est à partir d'une série d'images (1 en RGB ou 3 en niveau de gris sous éclairage particulier) de détecter le nombre d'objets d'une forme et d'une couleur particulière et de valider si les pièces requises pour un assemblage sont bien présentes (simuler le packaging d'un produit fini - 6 pièces de forme et de couleurs distinctes par exemple).

La validation du packaging final devra se faire sans l'intervention de l'être humain mais un affichage propre des couleurs devra être fait pour le contrôle par le/la manipulateur$\cdot$trice.

Bonus : mesure de la conformité de la taille des pièces ?


\newpage	
	
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    Objectifs

\section{Objectifs du bloc}

L'objectif principal de ce bloc est de découvrir les \textbf{différents paramètres impactant la qualité d'une prise d'image par une caméra} dans un environnement industriel ainsi que la \textbf{manipulation d'images numériques}, à l'aide de la bibliothèque \textbf{OpenCV}.

On s'intéressera à l'impact du \textbf{temps d'intégration}, de la \textbf{résolution} de la caméra et de l'\textbf{éclairage} (couleur et intensité) de la scène et des objets sur l'image résultante. 

\noindent \rule{\linewidth}{1pt}


\textit{Cette étude sera complétée par un TP plus détaillé sur les bruits associés à l'utilisation des caméras CMOS en 2ème année du cycle ingénieur à Palaiseau.}

\textit{Des cours et des projets autour du traitement d'images sont également proposés dans les prochaines années de formation, quelque soit le site. Ces deux séances sont une introduction à ces modules plus avancés.}

\textit{Le bloc \textbf{Images et OpenCV} (facultatif) de ce module permet d'aller également plus loin dans le pré-traitement des images et leur manipulation.}



\newpage
\strut % empty page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    Séance 1 détaillée

\begin{minipage}[c]{.25\linewidth}
	\includegraphics[width=4cm]{images/Logo-LEnsE.png}
\end{minipage} \hfill
\begin{minipage}[c]{.4\linewidth}

\begin{center}
\vspace{0.3cm}
{\Large \textsc{Interfaçage Numérique}}

\medskip

6N-047-SCI \qquad \textbf{\Large Bloc Caméra}

\end{center}
\end{minipage}\hfill

\vspace{0.5cm}

\noindent \rule{\linewidth}{1pt}

{\noindent\Large \rule[-7pt]{0pt}{30pt} \textbf{Séance 1} / Modélisation primaire d'une chaine d'acquisition} 

\noindent \rule{\linewidth}{1pt}

Ce bloc de travaux pratiques utilise un \textbf{banc de vision industrielle} avec une lampe de type Effi-Ring RGB, une caméra Basler et une interface développée en \textbf{Python} (\textit{PyQt6}) et qui utilise des fonctionnalités de la bibliothèque \textbf{OpenCV}.

Les documentations de la caméra et de l'éclairage sont disponibles aux adresses suivantes : 

\begin{itemize}
	\item Basler \textbf{a2A 1920 - 160ucBAS} : \href{https://docs.baslerweb.com/a2a1920-160ucbas#specifications}{https://docs.baslerweb.com/a2a1920-160ucbas\#specifications}
	\item \textbf{Effi-Ring} : \href{https://www.effilux.com/fr/produits/annulaire/effi-ring}{https://www.effilux.com/fr/produits/annulaire/effi-ring}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    Déroulement détaillé

%%%%%%%%%%%%%    Etape 0
\section{Etape 0 / Prendre en main l'interface de pilotage}

\begin{center} \textbf{\textit{Temps conseillé : 30 min}} \end{center}

\Manip Lancer l'application CMOS\_Machine\_Vision depuis le bureau des ordinateurs.

La dernière version officielle est sur le dépôt GitHub suivant : 

https://github.com/IOGS-LEnsE-ressources/machine-vision-gui  (version Basler)


\begin{center}
	\includegraphics[width=0.9\textwidth]{./images/camera_gui.png}
\end{center}


\Manip Ouvrir la première caméra disponible dans l'onglet \textsc{Image ou Caméra} / \textsc{Sélectionner une caméra} / \textsc{Ouvrir la première caméra}.

\subsection{Eclairage et zone d'intérêt}

\Manip Allumer l'éclairage annulaire du banc (trois couleurs). 

\Quest Quelle couleur d'éclairage obtient-on ?

\Manip Placer un objet (un cube de couleur par exemple) dans le champ de la caméra.

\Manip Ajuster la zone d'intérêt (ou \textit{Area of Interest} - AOI) à l'aide de l'onglet \textsc{Zone d'Intérêt} pour ne sélectionner qu'une partie de l'image autour de l'objet.

\Quest Que pouvez-vous dire des deux histogrammes affichés ? Quelles sont les valeurs minimale et maximale prises par les pixels de la caméra ? Quelle est alors la résolution de la caméra utilisée ?

\medskip

\Quest A quoi servent les deux bagues présentes sur l'objectif ?


\noindent \rule{\linewidth}{1pt}

\textit{\textbf{Pour la suite du TP, on s'assurera de prendre une zone d'intérêt à peu près centrée dans l'image et d'une taille d'environ 500 par 500 pixels.}}


\newpage
%%%%%%%%%%%%%    Etape 1
\section{Etape 1 / Décrire le rôle des principales caractéristiques d'une caméra CMOS}

\begin{center} \textbf{\textit{Temps conseillé : 30 min}} \end{center}


\subsection{Temps d'intégration}

\Manip Dans l'onglet \textsc{Image ou Caméra}, sélectionner un \textit{Black Level} de 0. Modifier le temps d'intégration de la caméra.

\Quest Que constatez-vous sur l'histogramme de l'image ? Sur la moyenne et l'écart-type de la répartition de la luminosité des pixels ?

\Manip Placer un cache devant la caméra (ou Prévoir une caméra annexe avec cache - pour éviter d'enlever l'objectif et mettre un cache...) pour vous placer dans l'obscurité.

\Quest La répartition de la luminosité perçu par chaque pixel est-elle uniforme ? Que pouvez-vous en conclure ?


\subsection{Echantillonnage et quantification}

\Manip Replacer l'objectif sur la caméra. Placer des objets dans le champ de la caméra. Sélectionner une zone d'intérêt d'environ 500 pixels par 500 pixels autour de l'objet à visualiser. Ajuster le temps d'intégration pour obtenir une image non saturée avec un éclairage blanc. 

\Manip Dans la section \textsc{Quantif./Echant.}, modifier la profondeur de quantification des informations. 

\Quest Que peut-on conclure sur l'effet de la quantification sur l'image ? Vous pourrez vous appuyer sur l'histogramme pour analyser le résultat.

\Manip De la même façon, dans la section \textsc{Quantif./Echant.}, modifier le nombre de pixels de sous-échantillonnage. 

On parle ici d'un phénomène de \textbf{binning}. La résolution de l'image est "dégragée" numériquement dans ce cas et les nouveaux pixels affichés sont la moyenne de N x N pixels de l'image initiale.

\textit{Dans le cas présent, ce phénomène peut simuler le changement de résolution de la caméra sur l'acquisition d'une image numérique.}

\Quest Que peut-on conclure sur l'effet de la résolution de la caméra sur l'image ? Vous pourrez vous appuyer sur l'histogramme pour analyser le résultat.


\newpage
%%%%%%%%%%%%%    Etape 2
\section{Etape 2 / Analyser l'impact des outils de base de la manipulation d'images}

\begin{center} \textbf{\textit{Temps conseillé : 60 min}} \end{center}

Dans cette section, nous allons nous intéresser à quelques fonctionnalités permettant de \textbf{manipuler des images} pour les rendre utilisables : suppression du bruit, seuillage, amélioration du contraste...

\subsection{Contraste et Luminosité}

\Manip Placer un objet dans le champ de la caméra. Sélectionner une zone d'intérêt d'environ 500 pixels par 500 pixels autour de l'objet à visualiser. Ajuster le temps d'intégration pour obtenir un histogramme dont le pixel maximum a une valeur de l'ordre des 2/3 de la valeur maximale de la caméra. 

\Manip Dans la section \textsc{Pré-Traitement}, puis sous-section \textsc{Contraste / Luminosité}, modifier les valeurs de contraste et de luminosité de l'image.

\Quest Quelles sont les opérations mathématiques réalisées sur les pixels par ces deux fonctionnalités ? Vous pourrez vous appuyer sur les histogrammes des images brutes et modifiées pour analyser vos résultats.

\Manip Dans la sous-section \textsc{Amélioration du Contraste}, tester l'effet des deux curseurs.

\Quest Proposer une interprétation de l'opération effectuée sur chacun des pixels.


\subsection{Seuillage}

\Manip Dans la section \textsc{Pré-Traitement}, puis sous-section \textsc{Seuillage}, sélectionner le seuillage \textsl{Normal} et modifier la valeur du seuil.

\Quest Que pouvez-vous conclure sur l'intérêt du seuillage ? Vous pourrez essayer avec des objets de taille, de forme et de couleurs différentes.

\Manip Tester également le mode \textsl{Inversé} et \textsl{Double}.

\Quest Que pouvez-vous conclure sur ces deux modes ?
 

\subsection{Filtrage}

\Manip Dans la section \textsc{Filtres}, puis sous-section \textsc{Filtre de lissage}, sélectionner le filtre \textsl{Blur Moyen} et un noyau de taille 15.

\Quest Que se passe-t-il sur l'image ? Vous pourrez également vous appuyer sur la différence entre l'image de base et l'image modifiée, en cliquant sur l'option \textsl{Image - Effet}, pour analyser les effets sur l'image.

\Quest Quel est l'effet de la taille du noyau sur le filtrage ?

\Quest Qu'en est-il avec le filtre de type \textsl{Médian} ? 

\medskip

\textit{Les aspects théoriques liés au filtrage de données (signaux et images) sont abordés dans les modules \textsc{Maths et Signal} (semestre 5) et \textsc{Traitement du Signal} (semestre 6).}

\textit{La mise en oeuvre de ces filtres sur des images sera abordée en TD de ce module et également dans des modules de traitement d'images dans vos prochaines années de formation.}


\newpage
%%%%%%%%%%%%%    Etape 3
\section{Etape 3 / Analyser l'impact du choix de la couleur de l'éclairage}

\begin{center} \textbf{\textit{Temps conseillé : 60 min}} \end{center}

L'acquisition d'image réalisée par l'interface est monochrome. Il est toutefois possible de détecter des couleurs dans les objets à analyser en jouant sur le choix de l'éclairage et sa couleur.


\subsection{Acquisition à des longueurs d'onde différentes}

Vous avez à votre disposition plusieurs objets de couleurs différentes.

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/cubes.jpg}
\end{center}


\Manip Placer les cubes de 4 couleurs différentes dans le champ de la caméra. Placer également la surface plane noire dans le champ.

\Manip Eclairer successivement la scène en rouge, puis en bleu, puis en vert. Visualiser les images acquises ainsi que les histogrammes associées.

\textit{Il est préférable de faire ces analyses dans une environnement sombre, voir dépourvu de tout éclairage parasite.}

\Quest A partir des histogrammes, proposer une méthode de détection des couleurs des objets.

\Manip Tester votre méthode. Faire valider par un$\cdot$e enseignant$\cdot$e.

\medskip

\Manip Placer à présent la feuille qui présente 3 carrés de couleurs différentes et un dessin.

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/dessin.jpg}
\end{center}

\Quest Proposer une méthode de détection de ces couleurs, afin de n'extraire qu'un des carrés.

\Manip Tester votre méthode. Faire valider par un$\cdot$e enseignant$\cdot$e.

\Quest Quelle lettre se cache dans votre dessin ? 

\textit{Vous pouvez également demander l'oeuvre de Thierry A. et rechercher la lettre mystère...}



\newpage
%%%%%%%%%%%%%    Etape 4
\section{Etape 4 / Modélisation simple d'une chaine d'acquisition}





\newpage
\subsection{Ouverture d'une image avec OpenCV}

Pour ouvrir les images, nous allons utiliser la bibliothèque \textbf{OpenCV} (plus de détails dans la séance 2 de ce TP). Nous utiliserons la version développée pour le langage Python.

Pour installer cette extension sous Python, il faut exécuter la commande suivante dans un terminal (ou Invite de commande sous Windows) :

\begin{lstlisting}
pip install opencv-python
\end{lstlisting}

\medskip

Si l'on souhaite ouvrir l'image \textsl{test.png} stockée dans le même que votre script en Python, à l'aide de la bibliothèque OpenCV et l'afficher, il faut utiliser les instructions suivantes :

\begin{lstlisting}
import cv2
from matplotlib import pyplot as plt

grayscale_image = cv2.imread('test.png', cv2.IMREAD_GRAYSCALE)
plt.imshow(grayscale_image, cmap='gray')
plt.show()
\end{lstlisting}

L'objet \textsl{grayscale\_image} est une matrice en deux dimensions, issue de la bibliothèque \textsl{Numpy}. Il est alors possible de faire des calculs sur cette matrice : moyenne, somme, addition et autres opérations mathématiques avec d'autres matrices...).

Il sera également possible d'appliquer d'autres fonctionnalités de pré-traitement ou de filtrage. Ces notions seront abordées dans la séance de TP suivante.


\subsection{Traitement avec OpenCV}

\Manip Réaliser les premières étapes du protocole afin de prendre des images exploitables du fond blanc choisi dans les 3 conditions d'éclairage proposées (R, G et B) ainsi que les images d'une main dans les mêmes conditions d'éclairage et d'acquisition pour la caméra.

\Manip Réaliser un script Python permettant d'ouvrir et d'afficher les 6 images.

\Manip Ajouter à ce script la possibilité de calculer l'image résultante du traitement proposé pour détecter les veines.

Il pourra être intéressant de calculer le logarithme de l'image résultante et d'afficher le résultat. Afin d'isoler plus facilement les pixels intéressants, vous pourrez utiliser l'interface de la caméra en chargeant l'image du logarithme de $I$ et utiliser le seuillage double pour isoler les objets intéressants...

\Quest Est-ce concluant ? Vous pourrez répéter ces étapes afin d'obtenir une meilleure qualité d'image (en jouant sur le temps d'intégration notamment).

\begin{center}
	\includegraphics[width=0.3\textwidth]{images/blood_detect.png}
\end{center}




\newpage
\strut % empty page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%    Séance 2 détaillée
\begin{minipage}[c]{.25\linewidth}
	\includegraphics[width=4cm]{images/Logo-LEnsE.png}
\end{minipage} \hfill
\begin{minipage}[c]{.4\linewidth}

\begin{center}
\vspace{0.3cm}
{\Large \textsc{Interfaçage Numérique}}

\medskip

6N-047-SCI \qquad \textbf{\Large Bloc Caméra}

\end{center}
\end{minipage}\hfill

\vspace{0.5cm}

\noindent \rule{\linewidth}{1pt}

{\noindent\Large \rule[-7pt]{0pt}{30pt} \textbf{Séance 2} / Manipulation d'images (OpenCV) et filtrage} 

\noindent \rule{\linewidth}{1pt}

Lors de cette séance, vous devrez écrire vos propres scripts en \textbf{Python} (avec l'IDE PyCharm par exemple) permettant de réaliser des opérations de base de manipulation d'images, à l'aide notamment de la célèbre bibliothèque \textbf{OpenCV}.

\section{Ressources}

Un tutoriel sur les bases d'OpenCV est disponible à l’adresse suivante : 

\href{https://iogs-lense-training.github.io/image-processing/}{https://iogs-lense-training.github.io/image-processing/}

Un \textbf{kit d'images} est disponible sur le site du LEnsE dans la rubrique \textit{Année / Première Année / Interfaçage Numérique S6 / Bloc Images et OpenCV / Kit d'images}. 


%%%%%%%%%%%%%    Etape 1
\section{Etape 1 / Ouvrir une image sous OpenCV}

\begin{center} \textbf{\textit{Temps conseillé : 20 min}} \end{center}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#open-an-image
}{\textit{Open an image}} - \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#display-an-image
}{\textit{Display an image}}
\end{mdframed}

\Manip Créer un nouveau projet sous PyCharm.

\Manip Impoter la bibliothèque \textbf{OpenCV2} (\textit{cv2}).

\medskip

\textit{Pour la suite, il est nécessaire de placer \textbf{tous les fichiers dans le même dossier} : votre script Python, les
différentes images et les bibliothèques de fonction (fichier \textsl{images\_manipulation.py})}

\Manip Ouvrir l'image \textsl{robot.jpg} du kit d'images fourni, au format RGB.

\Quest Quelle est la taille de l'image ? Quel est le type de données d'un élément ?

\Manip Ouvrir l'image \textsl{robot.jpg} du kit d'images fourni, en niveau de gris.

\Quest Quelle est la taille de l'image ? Quel est le type de données d'un élément ?


%%%%%%%%%%%%%    Etape 2
\section{Etape 2 / Calculer l'histogramme d'une image et l'afficher}

\begin{center} \textbf{\textit{Temps conseillé : 20 min}} \end{center}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#histogram-of-an-image}{\textit{Calculate the histogram}}
\end{mdframed}

\Manip Calculer l'histogramme de l'image précédente et l'afficher.

\medskip

\textit{Il peut être intéressant de \textbf{créer une fonction qui affiche automatiquement l'histogramme} d'une image à partir de ses données. Elle sera très utile dans la suite du TP pour voir l'impact des effets appliqués sur les images.}


%%%%%%%%%%%%%    Etape 3
\section{Etape 3 / Améliorer numériquement la qualité d'une image}

\begin{center} \textbf{\textit{Temps conseillé : 20 min}} \end{center}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#enhance-the-image-contrast-and-brightness
}{\textit{Enhance Contrast/Brightness}}
\end{mdframed}

Pour modifier le contraste et la luminosité d'une image, il faut appliquer une transformation linéaire \textbf{à chaque pixel} de l'image pouvant être exprimé mathématiquement comme suit :

$$P_{new} = \alpha \cdot P_{old} + \beta$$

où $\alpha$ est le facteur de contraste. Une valeur supérieure à 1 augmente le contraste, tandis qu'une valeur entre 0 et 1 le réduit.

$\beta$ est l'offset de luminosité. Une valeur positive rend l'image plus lumineuse, tandis qu'une valeur négative l'assombrit.

\medskip

On utilisera la fonction \textsl{cv2.convertScaleAbs()} pour modifier le contraste et la luminosité de l'image.

\bigskip

\Manip Ouvrir l'image \textsl{robot.jpg} du kit d'images fourni, en niveau de gris.

\Manip Modifier le contraste de l'image et comparer les histogrammes de l'image originale et de la version modifiée pour différente valeur de $\alpha$.

\Manip Modifier la luminosité de l'image et comparer les histogrammes de l'image originale et de la version modifiée pour différente valeur de $\beta$.


%%%%%%%%%%%%%    Etape 4
\section{Etape 4 / Appliquer un filtre moyenneur sur une image}

\begin{center} \textbf{\textit{Temps conseillé : 90 min}} \end{center}

La transformation précédente ne prend pas en compte les pixels voisins. Il est pourtant intéressant dans de nombreuses situations de capturer des relations spatiales entre les pixels.

Les filtres prenant en compte les pixels voisins exploitent les relations locales dans une image, ce qui est crucial pour des tâches comme la suppression de bruit, la détection de contours, l'extraction de caractéristiques, et l'amélioration de la qualité visuelle. Travailler sur des pixels isolés limite l'analyse à des informations ponctuelles, tandis que considérer les voisins permet une compréhension plus riche et contextuelle de l'image.

\subsection{Eléments structurants ou noyau}

Un \textbf{élément structurant} (ou noyau) est une petite matrice (généralement de taille et de forme prédéfinies, comme un carré, un disque, une ligne, etc.) qui sert de sonde pour inspecter et modifier les pixels d'une image.  

\begin{center}
	\includegraphics[width=\textwidth]{images/kernel.png}
\end{center}

Les éléments structurants jouent un rôle clé en traitement d'image, notamment dans les opérations de morphologie mathématique. Ces opérations sont principalement utilisées pour analyser et traiter des images binaires ou en niveaux de gris en modifiant leurs formes ou en extrayant des structures spécifiques.


\subsection{Filtre moyenneur gaussien}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv_blur.html#blur-with-opencv
}{\textit{Blur with OpenCV}}
\end{mdframed}

\Manip Créer un nouveau projet sous PyCharm.

\Manip Impoter la bibliothèque \textbf{OpenCV2} (\textit{cv2}).

\Manip Ouvrir l'image \textsl{bricks2.jpg} du kit d'images fourni, en niveau de gris.

\Manip Appliquer un filtre gaussien (fonction \textsl{cv2.GaussianBlur()}) sur l'image précédente, de taille 5 x 5 pixels.

\Manip Afficher les deux images pour les comparer.

\Quest Comment peut-on montrer de manière objective les modifications apportées à l'image ? 

\Manip Mettre en oeuvre la méthode proposée.

\medskip

On se propose d'utiliser la fonction \textsl{compare\_blur\_fft()} fournie dans le fichier \textsl{images\_manipulation.py} pour comparer l'effet. 

Ce fichier est disponible sur le site du LEnsE dans la rubrique \textit{Année / Première Année / Interfaçage Numérique S6 / Bloc Images et OpenCV / Répertoire vers codes à tester}.

\Manip Tester cette fonction sur l'image précédente.

\Quest Etudier cette fonction. Quelles méthodes sont utilisées pour comparer les images ?

\Quest Quelle est la fonction réalisée par le filtre précédent ?

\medskip

On cherche à présent à voir l'impact du noyau sur l'image finale.

\Manip Tester la fonction précédente avec des noyaux de taille différente et comparer les résultats à la fois sur l'image obtenue mais également sur la transformée de Fourier.

\Quest Que pouvez-vous conclure sur l'impact de la taille du noyau ?


\subsection{Bruit sur une image}

On se propose d'étudier la fonction \textsl{generate\_gaussian\_noise\_image()} fournie dans le fichier \textsl{images\_manipulation.py}.

\Manip Tester l'exemple fourni dans le fichier \textsl{noise\_test1.py}.

\Quest Comment vérifier la distribution du bruit généré par cette fonction ?

\medskip

On se propose d'étudier la fonction \textsl{generate\_uniform\_noise\_image()} fournie dans le fichier \textsl{images\_manipulation.py}.

\Manip Tester l'exemple fourni dans le fichier \textsl{noise\_test2.py}.

\Quest La distribution du bruit généré par cette fonction est-elle uniforme ?

\Manip A l'aide de la fonction \textsl{generate\_gaussian\_noise\_image\_percent()}, générer un bruit gaussien de moyenne 30 et d'écart-type 20 sur 10\% de l'image \textsl{robot.jpg} ouverte précédemment en nuance de gris. Visualiser le résultat.

\medskip

\textit{Il peut-être nécessaire de normaliser les images bruitées afin que les \textbf{données entières} de chaque pixel soient comprises \textbf{entre 0 et 255}, afin que les images puissent être affichées correctement.} 

\Manip Tester la fonction \textsl{compare\_blur\_fft()} fournie dans le fichier \textsl{images\_manipulation.py} et comparer les résultats à la fois sur l'image obtenue mais également sur la transformée de Fourier.


%%%%%%%%%%%%%    Etape 5
\section{Etape 5 / Appliquer un filtre passe-haut sur une image}

\begin{center} \textbf{\textit{Temps conseillé : 60 min}} \end{center}

Le \textbf{filtre moyenneur} précédent permet de conserver les \textbf{éléments à basse fréquence spatiale} dans l'image. C'est une méthode intéressante pour supprimer des bruits ponctuels (des éléments isolés et donc "rapides"). Il est également possible en choisissant un autre élément structurant de réaliser l'opération complémentaire qui supprime le fond continu et ne conserve que les transitions de fréquence spatiale élevée (bords d'un objet par exemple).

\medskip

Il est possible d'utiliser la fonction \textsl{cv2.filter2D()} pour appliquer un noyau particulier sur une image.

\subsection{Opérateur de Roberts}

L'opérateur de Roberts est l'un des premiers filtres de \textbf{détection de contours}. Il repose sur la convolution avec deux petits noyaux 2x2, conçus pour approximer les dérivées en diagonale de l'image.

Les noyaux de convolution sont les suivants :

$$K_x = \begin{bmatrix}
+1 & 0 \\
0 & -1
\end{bmatrix},
\quad
K_y =
\begin{bmatrix}
0 & +1 \\
-1 & 0
\end{bmatrix}.
$$

Il est alors possible de calculer l'amplitude du gradient par l'opération suivante: 

$$\text{Amplitude} = \sqrt{G_x^2 + G_y^2}$$ où $G_x$ est le résultat de la convolution de l'image par le noyau $K_x$ et $G_y$ le résultat de la convolution de l'image par le noyau $K_y$.

Une forte amplitude indique un contour ou un bord marqué. Une faible amplitude indique une région où l'intensité est relativement constante.

\medskip

\Manip Ecrire un script qui permet d'appliquer cette opération sur l'image \textsl{bricks2.jpg} du kit d'images fourni, en niveau de gris.

\Manip Visualiser l'image originale, le résultat du filtrage selon X et le résultat du filtrage selon Y.

\Quest Que pouvez-vous conclure sur l'effet de ce filtre ?

\subsection{Opérateur de Sobel}

L'opérateur de Sobel permet de réaliser une opération similaire à celui de Roberts, mais en étant moins sensible aux bruits dans l'image, puisqu'il se base sur un noyau plus large et ainsi lisse l'image dans la direction perpendiculaire au gradient mesuré.

Les noyaux de convolution sont les suivants :

$$G_x = \begin{bmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1 
\end{bmatrix},
\quad
G_y =
\begin{bmatrix}
-1 & -2 & -1 \\
0 & 0 & 0 \\
1 & 2 & 1 
\end{bmatrix}.
$$

De la même façon que précédemment, on peut calculer l'amplitude du gradient par l'opération suivante : 

$$\text{Amplitude} = \sqrt{G_x^2 + G_y^2}$$ où $G_x$ est le résultat de la convolution de l'image par le noyau $K_x$ et $G_y$ le résultat de la convolution de l'image par le noyau $K_y$.

\medskip

\Manip Ecrire un script qui permet d'appliquer cette opération sur l'image \textsl{bricks2.jpg} du kit d'images fourni, en niveau de gris.

\Manip Visualiser l'image originale, le résultat du filtrage selon X et le résultat du filtrage selon Y.

\Quest Que pouvez-vous conclure sur l'effet de ce filtre ?

%%%%%%%%%%%%%    Etape 6
\section{Etape 6 / Appliquer un filtre par l'intermédiaire de la transformée de Fourier}

\begin{center} \textbf{\textit{Temps conseillé : 30 min}} \end{center}

On s'intéresse ici à la \textbf{transformée de Fourier discrète en deux dimensions} permettant de représenter l'image dans le domaine fréquentiel, plutôt que dans le domaine spatial.

\Manip Ouvrir l'image \textsl{robot.jpg} du kit d'images fourni, en niveau de gris.

\Manip Calculer la transformée de Fourier discrète de cette image à l'aide de la fonction \textsl{fft2()} de la bibliothèque \textbf{Numpy / FFT}. Afficher l'image et sa transformée de Fourier. \textit{Pensez à utiliser la fonction fftshift...}

\medskip

On se propose d'utiliser la fonction \textsl{circular\_mask()} fournie dans le fichier \textsl{images\_manipulation.py} pour appliquer un masque sur la transformée de Fourier de l'image. 

Ce fichier est disponible sur le site du LEnsE dans la rubrique \textit{Année / Première Année / Interfaçage Numérique S6 / Bloc Images et OpenCV / Répertoire vers codes à tester}.

\Manip Appliquer un masque circulaire sur la transformée de Fourier de l'image, à l'aide de la fonction \textsl{circular\_mask()}. Afficher le résultat.

\Manip Calculer l'image résultante à l'aide de la fonction \textit{ifft2()} de la bibliothèque \textbf{Numpy / FFT}. Afficher l'image résultante et la comparer à l'image originale.

\medskip

\Manip Ecrire et tester une fonction permettant de réaliser un masquage rectangulaire sur une image, dont on pourra préciser la largeur et la longueur, ainsi que le barycentre du rectangle (point d'intersection des diagonales).

\Quest Que se passe-t-il lorsque vous appliquez un masque rectangulaire (dont la largeur et la longueur sont différentes) sur une image ? Qu'en est-il d'un masque circulaire ?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% RESSOURCES COMPLEMENTAIRES	

\newpage
\begin{center}
	\begin{minipage}{2.5cm}
	\begin{center}
		\includegraphics[width=5cm]{images/Logo-LEnsE.png}
	\end{center}
\end{minipage}\hfill
\begin{minipage}{10cm}
	\begin{center}
	\textbf{Institut d'Optique Graduate School }\\[0.1cm]
    \textbf{Interfaçage Numérique}


	\end{center}
\end{minipage}\hfill


\vspace{2cm}


{\Large \bfseries \textsc{Interfaçage Numérique}} \\[0.5cm]
{\large \bfseries Travaux Pratiques} \\[0.2cm]
Semestre 6

\vspace{1cm}

% Title
\rule{\linewidth}{0.4mm} \\[0.4cm]
{ \Large \bfseries\color{violet_iogs} Ressources \\[0.4cm] }
\rule{\linewidth}{0.4mm} \\[1cm]
{\large Bloc Caméra}

\end{center}

\vspace{3cm}



\textbf{\large Liste des ressources}
\begin{itemize}
	\item \hyperref[doc:image_proc]{Camera and sensor / Key concepts}
\end{itemize}

\vfill

\newpage
\strut % empty page


\includepdf[pages={1,3}, nup=1x2, pagecommand={\section{\texorpdfstring{\hspace{-1em}}{Camera and sensor}}}\label{doc:image_proc}]{../docs/Cameras.pdf}
\includepdf[pages={7,10-14,16,19,22,23,24,25,8}, nup=1x2]{../docs/Cameras.pdf}
	
	
	

\newpage
\section{Primitives}

En traitement d'image, les \textbf{primitives} sont les éléments fondamentaux ou les structures de base qui composent une image, sur lesquels des algorithmes peuvent opérer pour effectuer des analyses ou des traitements. 

Les primitives servent de \textbf{points de départ} pour la reconnaissance d'objets, l'analyse de scène, la segmentation d'image ou la reconstruction 3D. Par exemple, pour détecter un visage dans une image, l'algorithme peut commencer par identifier des primitives simples comme les yeux (points ou régions sombres), puis les relier pour former une structure cohérente.

\bigskip

On peut distinguer 3 catégories de primitives.

\subsection{Primitives de bas niveau}

Ce sont les entités les plus simples extraites directement des pixels de l'image. 

Par exemple :

\begin{itemize}
	\item Points : des pixels isolés ou des points d'intérêt
	\item Lignes ou segments : des ensembles de pixels alignés détectés par des algorithmes de détection de bord
	\item Contours : les frontières des objets définis par des changements d'intensité ou de couleur
	\item Régions : des groupes de pixels connectés ayant des propriétés similaires.
\end{itemize}


\subsection{Primitives de niveau intermédiaire}

Ces primitives sont obtenues en combinant ou en analysant les primitives de bas niveau. 

Par exemple :

\begin{itemize}
	\item Formes géométriques : rectangles, cercles, polygones
	\item Lignes ou segments : des ensembles de pixels alignés détectés par des algorithmes de détection de bord
	\item Objets simples : identification d'objets à partir de leurs contours ou formes.
\end{itemize}

\subsection{Primitives de haut niveau}

Ces primitives sont plus abstraites et dépendent de la compréhension sémantique de l'image. 

Par exemple :

\begin{itemize}
	\item Objets complexes : reconnaissance d'éléments comme des personnes ou des animaux
	\item Relations spatiales : liens entre différents objets (par exemple, un objet en avant d'un autre).
\end{itemize}



\newpage
%%%%%%%%%%%%%    Etape 0
\section{Ouvrir une image sous OpenCV et afficher son histogramme}

\begin{center} \textbf{\textit{Temps conseillé : 30 min}} \end{center}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#open-an-image}{\textit{Ouvrir une image}} - \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#display-an-image
}{\textit{Afficher une image}} - \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#histogram-of-an-image}{\textit{Calculer l'histogramme}}
\end{mdframed}
 
\Manip Créer un nouveau projet sous PyCharm et impoter la bibliothèque OpenCV2.

\Manip Ouvrir et afficher l'image \textsl{robot.jpg} du kit d'images fourni, en niveau de gris.

\Quest Quelle est la taille de l'image ? Quel est le type d'un élément ?

\Manip Calculer l'histogramme de l'image et l'afficher.

\medskip

\textit{Il peut être intéressant de \textbf{créer une fonction qui affiche automatiquement l'histogramme} d'une image à partir de ses données. Elle sera très utile dans la suite du TP pour voir l'impact des effets appliqués sur les images.}


%%%%%%%%%%%%%    Etape 1
\section{Couleur vers niveau de gris}

\begin{center} \textbf{\textit{Temps conseillé : 30 min}} \end{center}

\Manip Ouvrir et afficher l'image \textsl{couleurs\_4.png} du kit d'images fourni, au format RGB.

\Quest Quelle est la taille de l'image ? Quel est le type de données d'un élément ?


\Quest Est-il possible d'afficher un histogramme de l'image ? 

\Manip Créer une copie de la matrice image (fonction \textsl{copy()} de Numpy). 

\Manip Forcer à 0 tous les pixels du canal bleu de la copie de l'image et afficher la nouvelle image.


\subsection{RVB vs Niveau de gris}

Une image RVB contient 3 canaux (Rouge, Vert, Bleu ou \textit{RGB} en anglais), tandis qu'une image en niveaux de gris n'en a qu'un. Une image en niveau de gris sera \textbf{3 fois plus rapide} à analyser qu'une image en couleur RVB mais toute notion de couleur sera alors perdue.

\begin{center}
	\includegraphics[width=0.8\textwidth]{images/images_array_gray_rgb.png}
\end{center}


La couleur des objets peut s'avérer inutile lorsqu'on cherche, par exemple, à détecter des formes particulières ou des contours dans une image.

De nombreux algorithmes d'analyse d'image ou de vision par ordinateur travaillent plus efficacement sur des images en niveaux de gris, permettant notamment d'uniformiser l'entrée des algorithmes et de réduire les informations redondantes liées à la couleur.

\subsection{Plusieurs méthodes de conversion}

Plusieurs méthodes existent pour passer d'une image RVB à une image en niveau de gris :

\begin{itemize}
	\item Calculer la \textbf{moyenne des valeurs} des trois canaux de couleur (Rouge, Vert, Bleu) pour chaque pixel.
	\item Utiliser des \textbf{poids spécifiques pour les canaux R, V et B}, basés sur leur contribution relative à la perception humaine.
	\item Convertir l'image dans un \textbf{autre espace de couleur}, comme YUV, HSL ou HSV, et extraire la composante de luminosité.
\end{itemize}

\subsubsection{Moyenne des canaux R,V,B}

Cette méthode est la plus simple. Chaque pixel de l'image en gris est la moyenne des pixels des canaux rouge, vert et bleu de l'image en couleur :

$$Pixel_{Gray} = \frac{Pixel_{R} + Pixel_{V} + Pixel_{B}}{3}$$ 

\Manip Créer une image en nuance de gris utilisant la méthode de la moyenne des trois canaux.

\Manip Afficher l'image résultante.


\subsubsection{Pondération en fonction de la perception humaine}

Cette méthode est une moyenne pondérée des valeurs des pixels R, V, B de l'image couleur : 

$$Pixel_{Gray} = 0.299 \cdot Pixel_{R} + 0.587 \cdot Pixel_{V} + 0.114 \cdot Pixel_{B}$$ 


Les coefficients de cette méthode proviennent de la sensibilité relative de l'œil humain aux différentes couleurs et ont été standardisés à l'origine pour la télévision analogique (NTSC). Ils sont aujourd'hui utilisés comme une approximation fidèle de la perception visuelle de la luminosité.

La méthode de conversion fournie par la bibliothèque OpenCV se base sur cette pondération.

\Manip Convertir l'image RVB en niveau de gris par l'instruction suivante :

\begin{lstlisting}
image_gray = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2GRAY)
\end{lstlisting}

\Manip Comparer les images obtenues par la moyenne classique et cette moyenne pondérée.


\subsubsection{Utilisation d'un espace colorimétrique différent}

L'espace colorimétrique RVB est très utilisé dans le domaine du numérique (affichage, acquisition d'images) pour sa facilité de mise en oeuvre.

Cependant, ce n'est \textbf{pas} le plus \textbf{adapté vis-à-vis de la perception humaine} où la luminance et la couleur sont séparées.

Des espaces comme YUV, YIQ, ou YCbCr séparent la composante de luminance (Y) des composantes de chrominance (U et V).

\Manip Convertir l'image RVB dans l'espace YUV par l'instruction suivante :

\begin{lstlisting}
image_yuv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2YUV)
\end{lstlisting}

\Manip Comparer alors l'image en niveau de gris obtenue par la méthode de moyennage pondérée et le canal Y de cette conversion.

\Quest Que pouvez-vous conclure sur la méthode de calcul utiliser pour la luminance (Y) ?

\medskip

\noindent \rule{\linewidth}{1pt}

Il existe d'autres espaces colorimétriques dans le domaine numérique. Voici un résumé non exhaustif :

\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Espace colorimétrique} & \textbf{Avantages} \\ \hline
\textbf{RGB} & Simple, utilisé pour les écrans et le rendu des couleurs. \\ \hline
\textbf{HSV / HSL} & Intuitif pour manipuler la couleur (teinte, saturation). \\ \hline
\textbf{YUV / YCbCr} & Sépare luminance et chrominance. \\ \hline
\textbf{CIE-Lab} & Uniformité perceptuelle, idéal pour mesurer les différences de couleur. \\ \hline
\textbf{CMY(K)} & Optimisé pour l'impression. \\ \hline
\textbf{XYZ} & Modèle basé sur la perception humaine. \\ \hline
\end{tabular}
\end{center}

\newpage
%%%%%%%%%%%%%    Etape 2
\section{Seuillage et binarisation}

\begin{center} \textbf{\textit{Temps conseillé : 30 min}} \end{center}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv.html#binarize-an-image}{\textit{Binarisation d'une image}} - \href{https://iogs-lense-training.github.io/python-for-science/contents/python_time.html}{\textit{Mesurer un temps d'exécution}}
\end{mdframed}

Le seuillage (ou binarisation) d'une image est une technique fondamentale en traitement d'image qui consiste à convertir une image en niveaux de gris en une image binaire, composée uniquement de deux niveaux (0 ou 1, ou encore noir et blanc).

\begin{center}
	\includegraphics[width=0.8\textwidth]{images/binarized_image.png}
\end{center}

\subsection{Méthode classique}

\Manip Calculer l'image binarisée de l'image initiale - robot.jpg - (en niveau de gris) avec un seuil de 120, par la méthode \textsc{cv2.THRESH\_BINARY}.

\Manip Afficher l'image résultante. 

\Manip Tester également pour différente valeur de seuil. Tester également la méthode \textsc{cv2.THRESH\_BINARY\_INV}.

\Quest Que pouvez-vous conclure sur l'utilisation de cette méthode.

\subsection{Méthode d'Otsu}

La \textbf{méthode d'Otsu} permet d'effectuer un seuillage global automatique d'une image. Cette méthode est idéale pour les images où les niveaux de gris des objets et de l'arrière-plan forment \textbf{deux classes} bien distinctes. Dans ce cas, le seuil optimal pour séparer les niveaux de gris en deux classes (par exemple, objets d'intérêt et arrière-plan) est trouvé automatiquement : le seuil doit minimiser la variance intra-classe (dispersion des niveaux de gris dans chaque classe) et maximiser la variance inter-classe (différence entre les classes).

Cependant, son efficacité peut être limitée dans des cas de bruit ou de complexité multimodale.

\medskip

Il est possible d'utiliser cette méthode à l'aide de la fonction \textsl{threshold()} d'OpenCV de la façon suivante : 

\begin{lstlisting}
otsu_val, binary_image = cv2.threshold(image_gray, 0, 255, 
					cv2.THRESH_BINARY + cv2.THRESH_OTSU)
\end{lstlisting}

\Manip Tester cette méthode sur l'image \textbf{robot.jpg}. Comparer alors l'image résultante avec la méthode classique de seuillage.

\Quest A quoi correspond la valeur \textsl{otsu\_val} ?

\textit{Vous pourrez aussi comparer les temps d'exécution des deux méthodes...}


%%%%%%%%%%%%%    Etape 3
\section{Erosion, dilatation et gradient}

\begin{center} \textbf{\textit{Temps conseillé : 60 min}} \end{center}

On se propose ici d'analyser l'impact de différents procédés de pré-traitements (érosion, dilatation et gradient) sur une image.

\medskip

Les pré-traitements à étudier sont à réaliser sur l'image \textsl{a\_letter\_noise.jpg} du kit d'images fourni. Vous pourrez utiliser la fonction \textsl{zoom\_array()} fournie dans le fichier \textsl{images\_manipulation.py} afin d'augmenter la taille des images à analyser. 

\medskip

Pour faciliter l'analyse des images, on propose le code suivant permettant d'afficher 3 images en parallèle sur un même graphique :

\begin{lstlisting}
fig, ax = plt.subplots(nrows=1, ncols=3)    
ax[0].imshow(image_data_1, cmap='gray')
ax[0].set_title('Title Image 1')  
ax[1].imshow(image_data_2, cmap='gray')
ax[1].set_title('Title Image 2')
ax[2].imshow(image_data_3, cmap='gray')
ax[2].set_title('Title Image 3')
\end{lstlisting}


\subsection{Opérations de pré-traitement}

Les opérations de pré-traitement dans le traitement d'images sont essentielles pour \textbf{améliorer la qualité des images} avant d'appliquer des algorithmes plus complexes, comme la segmentation, la détection d'objets ou la classification. Ces étapes de pré-traitement visent à \textbf{réduire le bruit} ou \textbf{améliorer la structure de l'image}. 

Parmi les opérations de pré-traitement classiques, on peut citer :

\begin{itemize}
	\item \textbf{Correction des couleurs} : Balance des blancs, Correction gamma, Amélioration de contraste...
	\item \textbf{Réduction de bruit} : Filtrage linéaire pour atténuer les bruits sans trop affecter les détails importants de l'image, Filtrage non linéaire pour éliminer les bruits impulsionnels, Filtrage anisotrope...
	\item \textbf{Opérations morphologiques} : érosion pour éliminer du bruit, dilatation pour combler des lacunes dans les objets, ouverture et fermeture pour enlever les petites anomalies ou remplir les petits trous dans une image
	\item \textbf{Filtrage fréquentiel} pour éliminer ou atténuer des fréquences particulières (comme des motifs de bruit répétitifs) 
\end{itemize}

\subsection{Eléments structurants d'une convolution (noyau)}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv_erod_dila.html#kernels-for-erosion-and-dilation}{\textit{Structuring Elements (kernels)}} 
\end{mdframed}

Les \textbf{transformations dites morphologiques} se basent sur l'application d'un \textbf{élément structurant} (ou noyau) que l'on va superposer sur chaque pixel de l'image. 

\Manip Générer un noyau en forme de croix de taille 3 par 3 pixels et afficher ce noyau.

\Quest Quel est le type de l'objet noyau résultant ?

\Manip Générer un second noyau en forme de carré de taille 3 par 3 pixels et afficher ce noyau.

\newpage
\subsection{Opérations d'érosion et de dilatation}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv_erod_dila.html#erosion-operation}{\textit{Erosion}} - \href{https://iogs-lense-training.github.io/image-processing/contents/opencv_erod_dila.html#dilation-operation}{\textit{Dilation}} 
\end{mdframed}

\Manip Appliquer une opération d'érosion sur l'image \textsl{a\_letter\_noise.jpg} avec, indépendamment, les deux noyaux précédemment générés. 

\Manip Afficher les deux images ainsi que l'image originale sur un même graphique pour les comparer.

\Manip De la même manière, utiliser une opération de dilatation sur cette même image à l'aide des deux noyaux précédemment générés. Afficher également un comparatif des images résultantes.

\Quest Que pouvez-vous conclure sur l'utilité des opérations d'érosion et de dilatation sur une image ?

\subsection{Opérations d'ouverture et de fermeture}

\begin{mdframed}[style=sidebar,frametitle={}]
Notions : \href{https://iogs-lense-training.github.io/image-processing/contents/opencv_open_close.html#opening-operation}{\textit{Opening}} - \href{https://iogs-lense-training.github.io/image-processing/contents/opencv_open_close.html#closing-operation}{\textit{Closing}} 
\end{mdframed}

\Manip Appliquer une opération d'ouverture (\textit{opening}) sur l'image \textsl{a\_letter\_noise.jpg} avec, indépendamment, les deux noyaux précédemment générés. 

\Manip Afficher les deux images ainsi que l'image originale sur un même graphique pour les comparer.

\Manip De la même manière, utiliser une opération de fermeture sur cette même image à l'aide des deux noyaux précédemment générés. Afficher également un comparatif des images résultantes.

\Quest Que pouvez-vous conclure sur l'utilité des opérations d'ouverture et de fermeture sur une image ?

% L'opération d'opening est utile pour supprimer les petits bruits tout en maintenant la forme et la taille des objets plus grands dans l'image.


\subsection{Opération de gradient}

Une autre opération, appelée \textbf{gradient}, calcule la différence entre une dilatation et une érosion sur une même image. 

Il est possible de la mettre en pratique à l'aide de l'instruction suivante :

\begin{lstlisting}
gradient_image = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)
\end{lstlisting}

\Manip Appliquer une opération de gradient sur l'image \textsl{robot.jpg} avec, indépendamment, les deux noyaux précédemment générés. 

\Manip Afficher les deux images ainsi que l'image originale sur un même graphique pour les comparer.

\Quest Que pouvez-vous conclure sur l'utilité de l'opération de gradient sur une image ?

% Le gradient met en évidence les bords des objets dans une image, ce qui peut être utile pour détecter des contours ou des transitions nettes dans l'intensité des pixels.

\newpage
%%%%%%%%%%%%%    Etape 4
\section{Lissage du bruit}

\begin{center} \textbf{\textit{Temps conseillé : 90 min}} \end{center}

\subsection{Générer du bruit sur des images}

On se propose d'étudier la fonction \textsl{generate\_gaussian\_noise\_image()} fournie dans le fichier 

\textsl{images\_manipulation.py}.

\Manip Tester l'exemple fourni dans le fichier \textsl{noise\_test1.py}.

\Quest Comment vérifier la distribution du bruit généré par cette fonction ?

\medskip

On se propose d'étudier la fonction \textsl{generate\_uniform\_noise\_image()} fournie dans le fichier 

\textsl{images\_manipulation.py}.

\Manip Tester l'exemple fourni dans le fichier \textsl{noise\_test2.py}.

\Quest La distribution du bruit généré par cette fonction est-elle uniforme ?

\Manip A l'aide de la fonction \textsl{generate\_gaussian\_noise\_image\_percent()}, générer un bruit gaussien de moyenne 30 et d'écart-type 20 sur 10\% de l'image \textsl{robot.jpg} ouverte précédemment en nuance de gris. 


\subsection{Comparer différents filtres de lissage}

On se propose à présent d'analyser l'impact de différents filtres de lissage  (flou gaussien, filtre médian et filtre moyenneur) sur une image.

Pour ces trois types de filtres, répéter les étapes suivantes :

\Manip Appliquer une opération de lissage avec le filtre souhaité sur l'image \textsl{robot.jpg} avec un noyau de 15 x 15 pixels.

\Manip Stocker dans une matrice la différence entre l'image originale et l'image lissée. Afficher l'image originale, l'image lissée et la différence de deux images sur un même graphique pour les comparer.

\Manip Ajouter du bruit gaussien sur l'image et appliquer à nouveau le filtre gaussien. Afficher l'image originale, l'image lissée et la différence de deux images sur un même graphique pour les comparer.

\Quest Que pouvez-vous conclure sur l'utilité d'un tel filtre ?

\textit{Vous pourrez également regarder l'impact de la taille du noyau sur l'image lissée finale.}


\subsubsection{Filtre de type gaussien}

\Manip Appliquer une opération de lissage de type \textsc{Gaussian Blur} sur l'image \textsl{robot.jpg} avec un noyau de 15 x 15 pixels (\textsl{cv2.GaussianBlur}).

\subsubsection{Filtre de type médian}

\Manip Appliquer une opération de lissage de type \textsc{Median Blur} sur l'image \textsl{robot.jpg} avec un noyau de 15 x 15 pixels (\textsl{cv2.medianBlur}).

\subsubsection{Filtre de type moyenneur (mean ou box)}

\Manip Appliquer une opération de lissage de type \textsc{Averaging Blur} sur l'image \textsl{robot.jpg} avec un noyau de 15 x 15 pixels (\textsl{cv2.blur}).


\newpage	
%%%%%%%%%%%%%    Etape 5
\section{Isoler des éléments d'une image grâce à des opérations linéaires}

\begin{center} \textbf{\textit{Temps conseillé : 90 min}} \end{center}

Les opérateurs d'érosion et de dilatation permettent d'extraire des informations particulières dans l'image à partir du moment où les éléments structurants (noyaux de convolution) sont judicieusement choisis.

On va chercher ici à détecter les lignes horizontales et verticales de l'image \textsl{\textit{forms\_opening\_closing.png}} :

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/forms_opening_closing_inv.png}
\end{center}


\Manip Tester l'exemple fourni dans le fichier \textsl{line\_detection.py}.

\Manip Afficher les images aux différentes étapes du traitement.

\Quest Analyser les différentes phases du traitement. Quelle est la forme du noyau utilisé ? Quel est l'impact de sa taille sur les éléments détectés ?

\Manip A partir de l'exemple précédent, écrire un script qui permet de détecter les lignes verticales de cette image et afficher le résultat.

\Manip Tester ces deux exemples sur d'autres images.


\newpage
%%%%%%%%%%%%%    Etape 6
\section{Détecter des contours et des points d'intérêt}

\begin{center} \textbf{\textit{Temps conseillé : 90 min}} \end{center}

Le traitement numérique des images, de manière automatisée, vise souvent à \textbf{extraire des informations} d'une image en se basant sur de la {détection des caractéristiques} très spécifiques (ou \textit{features detection}) : 

\begin{itemize}
	\item détection de coins (Harris...)
	\item détection de bords (Canny, Sobel, Prewitt...)
	\item détection de régions homogènes (Laplacien, Difference-of-Gaussian...)
	\item détection de lignes (Hough...)
\end{itemize}

Il existe également d'autres procédés plus complexes, notamment invariants à l'échelle et à la rotation, pour détecter des objets dans une image (SIFT, SURF, BRIEF...) ou basés sur l'apprentissage profond (R-CNN...).


\subsection{Détection de coins par la méthode de Harris}

Un \textbf{coin} est un point dans une image où l'\textbf{intensité change fortement dans plusieurs directions} (par opposition aux bords, où l'intensité change principalement dans une seule direction).

La \textbf{méthode de Harris} détecte ces coins en analysant les variations locales d'intensité des pixels. Elle repose sur une matrice appelée matrice de structure (ou matrice de Harris), qui résume la distribution locale des gradients dans une région autour d'un pixel. Cette matrice est définie comme suit :

$$M = \begin{pmatrix}
I_x^2 & I_x \cdot I_y \\
I_x \cdot I_y & I_y^2\\
\end{pmatrix}$$


où : $I_x$ et $I_y$ sont les dérivées partielles de l'intensité de l'image dans les directions $x$ et $y$. Ces dérivées sont calculées à l'aide d'un filtre Sobel ou similaire.


Pour chaque pixel, Harris calcule un score basé sur les valeurs propres ($\lambda_1$, $\lambda_2$) de $M$, en utilisant la formule suivante : 
$$R = det(M) - k \cdot (trace(M))^2$$
 
où : $det(M) = \lambda_1 \cdot \lambda_2$ est le déterminant de $M$, $trace(M) = \lambda_1 + \lambda_2$ est la trace de $M$ et $k$ est une constante (généralement entre 0.04 et 0.06).

Le score $R$ permet de distinguer les caractéristiques suivantes :

\begin{itemize}
	\item $R > 0$ : Coin (les deux directions présentent une variation significative) ;
	\item $R \approx 0$ : Région plate (pas de variation significative) ;
	\item $R < 0$ : Bord (variation dans une seule direction). 
\end{itemize}

\medskip

\Manip Ouvrir l'image \textsl{robot.jpg} du kit d'images fourni, en niveau de gris. 

\Manip Appliquer le code suivant sur l'image ainsi ouverte et afficher l'image résultante :

\begin{lstlisting}
image_harris = cv2.cornerHarris(image_gray, 2, 3, 0.04)
# result is dilated for marking the corners, not important
# image_harris = cv2.dilate(image_harris, None)
# Threshold for an optimal value
image_gray[image_harris > 0.01 * image_harris.max()] = 0
\end{lstlisting}

\Manip Afficher également le résultat de la méthode \textsl{cornerHarris()}.

\Quest Que pouvez-vous conclure sur l'intérêt de la méthode de Harris ? Vous pourrez faire varier certains paramètres de la méthode \textsl{cornerHarris()}...


\subsection{Détection de contour par la méthode de Canny}

La méthode de Canny est un algorithme classique utilisé pour \textbf{détecter les bords} dans une image. On cherche ici à calculer des gradients d'intensité $G_x$ et $G_y$ pour estimer les changements d'intensité dans les directions horizontale et verticale (souvent à l'aide de filtres Sobel).

On peut alors définir l'amplitude du gradient $G$ et sa direction $\theta$ par les formules suivantes : 

$$G = \sqrt{G_x^2 + G_y^2}$$

$$\theta = arctan \frac{G_y}{G_x}$$

 
On souhaite ensuite ne conserver que les pixels correspondant aux crêtes (maxima locaux) des gradients dans la direction du gradient. Pour chaque pixel, on compare l'amplitude du gradient avec celles des pixels adjacents dans la direction $\theta$. Si ce n'est pas un maximum, le pixel est supprimé.

Puis deux seuils ($T_{haut}$ et $T_{bas}$) sont alors appliqués pour classifier les pixels :

\begin{itemize}
	\item $G > T_{haut}$ : pixels forts
	\item $T_{bas} < G < T_{haut}$ : pixels faibles
	\item $T_{bas} > G$ : pixels supprimés
\end{itemize}

Enfin, on détecte les pixels faibles qui sont connectés à des pixels forts. Ces ensembles sont conservés comme bords. Cela permet de relier les fragments de bords discontinus tout en éliminant les bords isolés ou bruités.

\medskip

\Manip Ouvrir l'image \textsl{robot.jpg} du kit d'images fourni, en niveau de gris. 

\Manip Appliquer le code suivant sur l'image ainsi ouverte et afficher l'image résultante :

\begin{lstlisting}
edges = cv2.Canny(image_gray, min_value, max_value)
\end{lstlisting}

\medskip

Pour en savoir plus sur l'algorithme de détection de contours de Harris : 

\href{https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html}{https://docs.opencv.org/4.x/da/d22/tutorial\_py\_canny.html} 


\newpage
%%%%%%%%%%%%%    Etape 7
\section{Segmenter une image par la méthode de Watershed}

\begin{center} \textbf{\textit{Temps conseillé : 90 min}} \end{center}

La méthode de \textbf{Watershed} (littéralement "ligne de partage des eaux") est une technique de \textbf{segmentation d'image} utilisée pour diviser une image en différentes régions ou objets. Elle est basée sur l'idée de traiter l'image comme une topographie où les niveaux de gris représentent des altitudes. L'objectif est de séparer les régions connectées tout en identifiant les limites précises entre elles.

Cette méthode nécessite au préalable quelques étapes de pré-traitement, basées sur les méthodes vues au cours de ce TP. 

L'algorithme complet, que vous allez mettre en oeuvre à présent, est le suivant :

\begin{enumerate}
	\item Ouverture de l'image en niveau de gris et seuillage (par la méthode d'Otsu)
	\item Réduction du bruit (ouverture morphologique)
	\item Identification des zones inconnues (ni fond, ni objets)
	\item Identification des objets	
	\item Application de l'algorithme de Watershed
\end{enumerate}

Pour le test et la compréhension de l'algorithme, il est intéressant d'afficher les images obtenues à chaque étape et de comprendre les différences avec l'étape précédente.
	
\subsection{Ouverture et seuillage}

\Manip Ouvrir l'image \textsl{bricks2.jpg} du kit d'images fourni, en niveau de gris. Puis appliquer un seuillage selon la méthode d'Otsu.

\subsection{Réduction du bruit}

\Manip Créer un élément structurant elliptique (\textsl{cv2.MORPH\_ELLIPSE}) de taille 3 par 3.

\Manip Utiliser ce noyau pour réaliser une opération morphologique d'ouverture sur l'image binaire précédemment obtenue. 

\subsection{Identification des zones inconnues}

Afin de pouvoir distinguer les objets sur l'image et ainsi les extraire du fond, nous allons chercher à séparer le fond (\textit{background}) des objets (\textit{foreground}).

\Manip Tester le code suivant sur l'image obtenue lors de l'étape précédente :

\begin{lstlisting}
# sure background area
sure_bg = cv2.dilate(opening, kernel, iterations=1)

# Finding sure foreground area
k_dist = 0.4
dist_transform = cv2.distanceTransform(opening, cv2.DIST_L1, 5)
ret, sure_fg = cv2.threshold(dist_transform, 
			k_dist * dist_transform.max(), 255, 0)

# Finding unknown region
sure_fg = np.uint8(sure_fg)
unknown = cv2.subtract(sure_bg, sure_fg)
\end{lstlisting}

\Quest A quoi correspondent les images : \textsl{sure\_bg}, \textsl{sure\_fg}, \textsl{unknown} et \textsl{dist\_transform} ?

\Quest Que se passe-t-il en modifiant le coefficient \textsl{k\_dist} ?

\subsection{Identification des objets}

Les pixels connectés dans les zones identifiées précédemment (\textsl{sure\_fg}) peuvent alors être considérés comme faisant partie d'un même objet. Il s'agit à présent de les identifier comme étant des objets différents.

\Manip Tester le code suivant sur l'image obtenue lors de l'étape précédente :

\begin{lstlisting}
# Marker labelling
ret, markers = cv2.connectedComponents(sure_fg)
# Add one to all labels so that sure background is not 0, but 1
markers = markers + 1
# Now, mark the region of unknown with zero
markers[unknown == 255] = 0
\end{lstlisting}

\Manip Afficher l'image \textsl{markers}.

\Quest A quoi correspond-elle ? La détection des objets est-elle optimale ? Pourquoi ?

\subsection{Application de l'algorithme de Watershed}

\Manip Tester le code suivant sur l'image obtenue lors de l'étape précédente (\textsl{markers}) et sur l'image RGB originale (\textsl{image\_rgb}) :

\begin{lstlisting}
image_rgb2 = image_rgb.copy()
markers2 = cv2.watershed(image_rgb, markers)
image_rgb[markers2 == -1] = [255, 0, 0]
\end{lstlisting}

\Quest A quoi sert la première ligne de ce code ?

\Manip Comparer l'image originale, l'image marquée par la méthode de Watershed et l'image finale.

\Quest Concluer sur l'intérêt d'un tel procédé et sur les paramètres qui peuvent modifier le résultat final.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% RESSOURCES COMPLEMENTAIRES		

\newpage
\begin{center}
	\begin{minipage}{2.5cm}
	\begin{center}
		\includegraphics[width=5cm]{images/Logo-LEnsE.png}
	\end{center}
\end{minipage}\hfill
\begin{minipage}{10cm}
	\begin{center}
	\textbf{Institut d'Optique Graduate School }\\[0.1cm]
    \textbf{Interfaçage Numérique}


	\end{center}
\end{minipage}\hfill


\vspace{2cm}


{\Large \bfseries \textsc{Interfaçage Numérique}} \\[0.5cm]
{\large \bfseries Travaux Pratiques} \\[0.2cm]
Semestre 6

\vspace{1cm}

% Title
\rule{\linewidth}{0.4mm} \\[0.4cm]
{ \Large \bfseries\color{violet_iogs} Ressources \\[0.4cm] }
\rule{\linewidth}{0.4mm} \\[1cm]
{\large Bloc Images et OpenCV}

\end{center}

\vspace{3cm}

\textbf{\large Liste des ressources}
\begin{itemize}
	\item \hyperref[doc:image_proc]{Image Processing / Key concepts}
\end{itemize}

\vfill

\newpage
\strut % empty page


\includepdf[pages={1,4}, nup=1x2, pagecommand={\section{\texorpdfstring{\hspace{-1em}}{Image Processing}}}\label{doc:image_proc}]{../docs/Image_Processing.pdf}
\includepdf[pages={5,8,12,14,16,17,18,20,21,22,23,24,25,31}, nup=1x2]{../docs/Image_Processing.pdf}

\end{document}


